{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d704f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.types import Send\n",
    "from typing import TypedDict\n",
    "import subprocess\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing_extensions import Annotated\n",
    "import operator\n",
    "import base64\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\")\n",
    "\n",
    "class State(TypedDict):\n",
    "\n",
    "    video_file: str\n",
    "    audio_file: str\n",
    "    transcription: str\n",
    "    summaries: Annotated[list[str], operator.add]\n",
    "    thumbnail_prompts: Annotated[list[str], operator.add]\n",
    "    thumbnail_sketches: Annotated[list[str], operator.add]\n",
    "    final_summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d1f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.resources.chat import CompletionsWithStreamingResponse\n",
    "\n",
    "\n",
    "def extract_audio(state: State):\n",
    "    output_file = state[\"video_file\"].replace(\"mp4\",\"mp3\")\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\",\n",
    "        state[\"video_file\"],\n",
    "        \"-filter:a\",\n",
    "        \"atempo=2.0\",\n",
    "        \"-y\",\n",
    "        output_file,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    return {\"audio_file\": output_file}\n",
    "\n",
    "def transcribe_audio(state: State):\n",
    "    client = OpenAI()\n",
    "    with open(state[\"audio_file\"], \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            response_format=\"text\",\n",
    "            file=audio_file,\n",
    "            language=\"ko\",\n",
    "            prompt=\"Jacqueline, Isabele\"\n",
    "        )\n",
    "    return {\"transcription\": transcript}\n",
    "\n",
    "def dispatch_summarizers(state: State):\n",
    "    transcription = state[\"transcription\"]\n",
    "    chunks = []\n",
    "    for i, chunk in enumerate(textwrap.wrap(transcription, 100)):\n",
    "        chunks.append({\"id\": i+1, \"chunk\": chunk})\n",
    "    return [Send(\"summerize_chunk\", chunk) for chunk in chunks]\n",
    "def summarize_chunk(chunk:str):\n",
    "    chunk_id = chunk[\"id\"]\n",
    "    chunk = chunk[\"chunk\"]\n",
    "\n",
    "    response = llm.invoke(\n",
    "        f\"\"\"Please summarize the following text:\n",
    "\n",
    "        Text: {chunk}\n",
    "        \"\"\"\n",
    "    )\n",
    "    summary = f\"[Chunk {chunk_id}] {response.content}\"\n",
    "    return {\n",
    "        \"summaries\": [summary],\n",
    "        }\n",
    "\n",
    "def mega_summary(state: State):\n",
    "    all_summaries = \"\\n\".join(state[\"summaries\"])\n",
    "    prompt = f\"\"\"\n",
    "    You are given multiple summaries of different chunks from a video transcription.\n",
    "    \n",
    "    Please create a comprehensive final summary that combines all the key points.\n",
    "\n",
    "    Individual summaries:\n",
    "\n",
    "    {all_summaries}\n",
    "    \"\"\"        \n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    return {\n",
    "        \"final_summary\": response.content\n",
    "    }\n",
    "\n",
    "def dispatch_artists(state: State):\n",
    "    return [\n",
    "        Send(\"generate_thumnails\", {\"id\": i, \"summary\": state[\"final_summary\"]}) \n",
    "        for i in [1,2,3,4,5]\n",
    "    ]\n",
    "\n",
    "def generate_thumnails(args):\n",
    "    concept_id = args[\"id\"]\n",
    "    summary = args[\"summary\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on this video summary, create a detailed visual prompt for a YouTube thumbnail.\n",
    "\n",
    "    Create a detailed prompt for generationg a thumbnail image that would attract viewers. Include:\n",
    "    - Main visual elements\n",
    "    - Color scheme\n",
    "    - Text overlay suggestions\n",
    "    - Overall composition \n",
    "\n",
    "    Summary: {summary}\n",
    "    \"\"\" \n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    thumbnail_prompt = response.content\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    result = client.images.generate(\n",
    "        model=\"gpt-image-1\",\n",
    "        prompt=thumbnail_prompt,\n",
    "        quality=\"low\",\n",
    "        moderation=\"low\",\n",
    "        size=\"auto\",\n",
    "    )\n",
    "\n",
    "    image_bytes = base64.b64decode(result.data[0].b64_json)\n",
    "    filename = f\"thumbnail_{concept_id}.jpeg\"\n",
    "\n",
    "    with open(filename, \"wb\") as file:\n",
    "        file.write(image_bytes)\n",
    "    \n",
    "    return {\n",
    "        \"thumbnail_prompts\": [thumbnail_prompt],\n",
    "        \"thumbnail_sketches\": [filename],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"extract_audio\", extract_audio)\n",
    "graph_builder.add_node(\"transcribe_audio\", transcribe_audio)\n",
    "graph_builder.add_node(\"summarize_chunk\", summarize_chunk)\n",
    "graph_builder.add_node(\"mega_summary\", mega_summary)\n",
    "graph_builder.add_node(\"generate_thumnails\", generate_thumnails)\n",
    "\n",
    "graph_builder.add_edge(START, \"extract_audio\")\n",
    "graph_builder.add_edge(\"extract_audio\", \"transcribe_audio\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"transcribe_audio\", dispatch_summarizers, [\"summarize_chunk\"])\n",
    "graph_builder.add_edge(\"summarize_chunk\", \"mega_summary\")\n",
    "graph_builder.add_conditional_edges(\"mega_summary\", dispatch_artists, [\"generate_thumnails\"])\n",
    "graph_builder.add_edge(\"generate_thumnails\", END)\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({\"video_file\": \"fun.mp4\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = \"자베리는 지금 뉴스 촬영에 푹 빠져있다. 무화지경에 빠져 챌린지 중인 자베리. 보고 있으면 피가 거꾸로 솟는다. 어머나! 나가요! 피가 다 엉췄... 자베라! 정말 진짜 진짜! 쟤 드라이밍이 크레이지! 한편, 귤락을 골라내는 섬세함이다. 영국의 그레이스도 K-드라마에 푹 빠졌다. 아, 정말 이거 다 빨라, 정말로! 진짜 그라이스다, 자베라 정말로 진짜! 어디론가 전화를 거는 그레이스도 여보, 오늘 올 때 마이크랑 그러면 사와. 당연히 레드지, 내가 피는 거! 몰라, 오늘은 그냥 시그널 피해야겠다. 오늘은 피해야 돼, 그냥 사와! 이날은 담배 없이는 버틸 수 없었다. 끊어요. 거의 영국인답게 차는 잊지 않는다. 그래, 이제는 나도 어쩔 수가 없어. 그냥 가, 그래! 불현듯 어디론가 전화를 거는 그레이스 노래를 이은미 창법으로 부른다. 여러분, Who is this? 어, 나야 유난다. 나 그레이스야. 어, 그레이스! 나 지금 오피스에서 스테인리스에서 있었는데 그레이스가 딱 전화를 했어. 유난다, 나 자베리 때문에 그냥 한국 가려고 그냥. 근데 한국을 갔다니? 어, 더 이상 미룰 수가 없을 것 같아. 그때 이제 한국에 에어앱 B&B인가 그거 무슨 사습지 같은 거 끝나고 그러셨지? 어, 맞아요, 언니. 우리 마이크맨 여기 비거리 힐 이제 우리 집 팔고 한국 가가지고 그거는 좀 하고 그랬거든. 어머, 그래? 소문에 린나 언니가 막 한국에서 브랜드처럼 산다고 막 그랬는데. 이제 린나 아니고 제클린이라니까! 이제 잘 있으니까 이상한 소리 좀 그만하고 다녀, 너! 제클린 험담에 속상해진 그레이스 쉿! 그레이스야! 뭐야? 다시 시작된 유난다의 이은미 창법 어디론가 또다시 전화를 거는 그레이스\\n'}\"\n",
    "import textwrap\n",
    "\n",
    "for i, chunk in enumerate(textwrap.wrap(transcription, 100)):\n",
    "    print(f\"chunk ID: {i+1}:{chunk}\")\n",
    "    print(\"======\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
